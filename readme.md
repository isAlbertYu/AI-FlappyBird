# 基于 Deep Q-Network 的 Flappy Bird游戏自动玩耍
## 主要环境依赖
Python 3.7.0<br>
Tensorflow 1.12.0<br>
Pygame 1.9.6<br>
游戏环境及资源：[Flappy Bird](https://github.com/sourabhv/FlapPyBird)

## 模型结构
使用深度卷积网络DCNN与全连接神经网络的结合，对Flappy Bird游戏强化学习进行了值函数逼近。使用游戏实时的数据（状态-动作-回报）作为值函数优化的训练集。
动作空间的动作数为2，可以进行one-hot编码，其中第一位表示无动作，第二位表示“flap”动作。DCNN对连续4帧游戏画面进行采样，最终网络的输出维数与动作空间的维数相等，即为一个2维数组，表示在该状态下所估算的值函数。
## 经验池
为了供训练使用数据集，构造了一个容量为50000的队列作为经验池，用于存放游戏的决策经验（当前状态，即时动作，即时回报，终止标志，下一步状态）。在最初10000步，先不对网络进行训练，而是直接使用初始化网络，并以 ϵ-greedy 策略进行决策，以产生初始经验，然后10000步后使用这些初始经验进行网络训练，同时以实时游戏经验更新经验池。
##  ϵ-greedy 策略
在 ϵ-greedy 策略中，该程序设置ϵ初始值为0.1，即以0.1的概率进行随机探索，此处之所以设置ϵ=0.1，是由于若ϵ太大，agent将会以较高的频率进行“flap”，会使得agent触碰上壁或上管道，导致游戏过早终止。这将会导致游戏状态单一，Q网络所能学习到的状态也较少，网络收敛较慢。
## 训练
训练迭代次数设置为3000000次，实际上可以无限制，次数越大，效果越好。训练使用数据为batch size为32的4通道图像，这是由于单一的一帧图像为静态图像，难以反映连续的状态与动作的变化。后期将探究使用更多帧的图像作为输入数据，并加深DCNN，观察效果。

#### 初始时效果：
<p align="center">
    <img src="assets/init.gif" width="20%">
</p>
一开始，就像蹒跚学步的婴儿，触障碰壁是必然的。

#### 70000次迭代后效果：

<p align="center">
    <img src="assets/70000.gif" width="20%">
</p>

效果仍然欠佳，但相比初始未训练时，已经勉强可以越过一个管道。随着迭代的次数继续推进，效果将越来越好。后期将公布迭代次数更大的更好效果。


